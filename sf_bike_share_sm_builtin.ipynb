{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SF Bay Area Bike Share using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use XGBoost to predict availability of bikes at various stations\n",
    "\n",
    "First we update the local version of Pandas before we import it\n",
    "\n",
    "*NOTE Once this installtion has completed, please restart the Kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pandas --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the required libraries and set up the figure parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7db090c04e9d682075d2230c535962ed40b4f684"
   },
   "outputs": [],
   "source": [
    "#Importing the required libraries and setting up the figure parameters\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import rcParams\n",
    "dark_colors = [\"#99D699\", \"#B2B2B2\",\n",
    "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
    "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
    "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
    "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
    "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
    "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
    "                (0.4, 0.4, 0.4)]\n",
    "rcParams['figure.figsize'] = (12, 9)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = \"white\"\n",
    "rcParams['axes.titlesize'] = 20      \n",
    "rcParams['axes.labelsize'] = 17.5\n",
    "rcParams['xtick.labelsize'] = 15 \n",
    "rcParams['ytick.labelsize'] = 15\n",
    "rcParams['legend.fontsize'] = 17.5\n",
    "rcParams['patch.edgecolor'] = 'none'\n",
    "rcParams['grid.color']=\"white\"   \n",
    "rcParams['grid.linestyle']=\"-\" \n",
    "rcParams['grid.linewidth'] = 1\n",
    "rcParams['grid.alpha']=1\n",
    "rcParams['text.color'] = \"444444\"\n",
    "rcParams['axes.labelcolor'] = \"444444\"\n",
    "rcParams['ytick.color'] = \"444444\"\n",
    "rcParams['xtick.color'] = \"444444\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make suure the Pandas version installed is >=1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace \\<REPLACE WITH YOUR BUCKET NAME\\> with your bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<REPLACE WITH YOUR BUCKET NAME>' # Replace with your bucket name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the datasets we will be using to your notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp s3://example-lab-artifacts/geospatial/dataset/ . --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the trips made and list of stations datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e116b51c1cfabda5eeabac39b62c110add81c919"
   },
   "outputs": [],
   "source": [
    "trips_df = pd.read_csv('./trip.csv')\n",
    "stations_df = pd.read_csv('./station.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the stations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb69a51bb0f9e9c3423b768eec3804d84fd10a5b"
   },
   "outputs": [],
   "source": [
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db8353b90fed9337426d3908b965b5e5348c89b9"
   },
   "outputs": [],
   "source": [
    "stations_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "424b971c6e767cf7b1bb869953e9860ad65720bf"
   },
   "outputs": [],
   "source": [
    "stations_df[\"lat\"] = stations_df[\"lat\"].apply(lambda x:str(x))\n",
    "stations_df[\"long\"] = stations_df[\"long\"].apply(lambda x:str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e291911cb5eafed8338107b684743846d411136"
   },
   "outputs": [],
   "source": [
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0338ba8c742fd0e49cb5b42d3dd843eea3345b87"
   },
   "outputs": [],
   "source": [
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b050d1ddaf19a8833608fe89444d59bcf1250024"
   },
   "outputs": [],
   "source": [
    "trips_df['start_date'] = pd.to_datetime(trips_df['start_date'])\n",
    "trips_df['end_date'] = pd.to_datetime(trips_df['end_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to join the trip and station datasets on station_id, alternatively same could be done through DataWrangler as explained last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09ded7151fc1e18c57fabc7e97b938b1e86f0ac0"
   },
   "outputs": [],
   "source": [
    "start_station_info = stations_df[[\"id\",\"lat\",\"long\"]]\n",
    "start_station_info.columns = [\"start_station_id\",\"start_lat\",\"start_long\"]\n",
    "end_station_info = stations_df[[\"id\",\"lat\",\"long\"]]\n",
    "end_station_info.columns = [\"end_station_id\",\"end_lat\",\"end_long\"]\n",
    "trips_df = trips_df.merge(start_station_info,on=\"start_station_id\")\n",
    "trips_df = trips_df.merge(end_station_info,on=\"end_station_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f0965029296e5c27fdfba30ffd02c864dae9447"
   },
   "outputs": [],
   "source": [
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a temporary dataset with fields of interest for plotting purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "634f687a3ddeea9daacc5d6cd7746bc70474827a"
   },
   "outputs": [],
   "source": [
    "plot_dict = dict()\n",
    "for index,row in trips_df.iterrows():\n",
    "    start_lat = row['start_lat']\n",
    "    start_long = row['start_long']\n",
    "    end_lat = row['end_lat']\n",
    "    end_long = row['end_long']\n",
    "    key = str(start_lat)+'_'+str(start_long)+'_'+str(end_lat)+'_'+str(end_long)\n",
    "    if key in plot_dict:\n",
    "        plot_dict[key] += 1\n",
    "    else:\n",
    "        plot_dict[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33cecb926bd1ee32c8b760d75b2cf918e3fc616d"
   },
   "outputs": [],
   "source": [
    "start_lat = []\n",
    "start_long = []\n",
    "end_lat = []\n",
    "end_long = []\n",
    "nb_trips = []\n",
    "for key,value in plot_dict.items():\n",
    "    start_lat.append(float(key.split('_')[0]))\n",
    "    start_long.append(float(key.split('_')[1]))\n",
    "    end_lat.append(float(key.split('_')[2]))\n",
    "    end_long.append(float(key.split('_')[3]))\n",
    "    nb_trips.append(int(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf5fb80b5369925becbfb1d3289647337f413e6b"
   },
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({\"start_lat\":start_lat,\"start_long\":start_long,\"end_lat\":end_lat,\"end_long\":end_long,\"nb_trips\":nb_trips})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "169475e3697f1957e5c7cbebf323e78aa057eeb7"
   },
   "outputs": [],
   "source": [
    "temp_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0bed93568f7d3d6bae14bb4622e10fc85c2766be"
   },
   "outputs": [],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the trip duration distribution. Shows that most trip durations are between 1-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f407f1c327d5b2aa615374cc1ab7a6c528439d6"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize = (10,7))\n",
    "ax1.grid(zorder=1)\n",
    "ax1.xaxis.grid(False)\n",
    "trip_dur = trips_df['duration'].values/60\n",
    "plt.hist(trip_dur, bins = range(0,45,2),density=True,zorder=0,color=dark_colors[1])\n",
    "plt.xlabel('Trip Duration (Minutes)')\n",
    "plt.ylabel('Percent of Trips')\n",
    "plt.title('Trip Duration Distribution')\n",
    "plt.figure(figsize=(15,12))\n",
    "hist, bin_edges = np.histogram(trip_dur, range(0,45,1), density=True)\n",
    "cum_trip_dur = np.cumsum(hist)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(1,45,1),cum_trip_dur,c=dark_colors[0])\n",
    "ax2.set_ylabel('Cumulative Proportion of Trips')\n",
    "ax2.grid(b=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b2005044a79b8d0008c6adc0d251166ffee0123"
   },
   "outputs": [],
   "source": [
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets parse out the date fields from the trips dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fce477667b5f21f76ee0152c971e1e1b68f90b58"
   },
   "outputs": [],
   "source": [
    "trips_df['week']=trips_df.start_date.dt.dayofweek\n",
    "trips_df['start_hour'] = trips_df.start_date.dt.hour\n",
    "trips_df['start_day'] = trips_df.start_date.dt.day\n",
    "trips_df['end_hour'] = trips_df.end_date.dt.hour\n",
    "trips_df['end_day'] = trips_df.end_date.dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now plot the trip distribution over time by hour in the day for week days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "952233eb0bc575f16390a69268b23fb8de9a45b4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "weekdaytrips_df = trips_df.loc[(trips_df.duration <= 7200) & (trips_df.week <5)]\n",
    "weekdaytrips_df.boxplot(column=\"duration\",by=\"start_hour\",figsize=(15,12))\n",
    "plt.ylim(0,3600)\n",
    "plt.ylabel('Trip Duration (Seconds)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.title('Trip Duration Distribution Over Time of Day (Week Days)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now plot the trip distribution over time by hour in the day for weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bdf19fc52c682327886e15ae694c558a981ef780"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "weekendtrips_df = trips_df.loc[(trips_df.duration <= 7200) & (trips_df.week >4)]\n",
    "weekendtrips_df.boxplot(column=\"duration\",by=\"start_hour\",figsize=(15,12))\n",
    "plt.ylim(0,3600)\n",
    "plt.ylabel('Trip Duration (Seconds)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.title('Trip Duration Distribution Over Time of Day (Weekend days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define a function to lower our memory footprint of the dataframe that will store the station status dataset (alomost 2GB). \n",
    "\n",
    "The function will iterate through all of the columns of the dataframe and modify the data type according to the data contained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "423070e04800e6b8cb1561510f647b8dcab3906f"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load the station status dataset and leverage the function we just defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5da7fce4375f453d5020f8fd18bc962de7f3571a"
   },
   "outputs": [],
   "source": [
    "status_df = reduce_mem_usage(pd.read_csv('./status.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a62e531f190ed34c07528b1583bd879e8f5f28a5"
   },
   "outputs": [],
   "source": [
    "status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd8522dba20e3e32a4eebbb34969aeef5fda3acb"
   },
   "outputs": [],
   "source": [
    "status_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94bcaab9b35cfba50ce2167affe6dff3c41d9ea5"
   },
   "outputs": [],
   "source": [
    "status_df.time = pd.to_datetime(status_df.time)\n",
    "status_df = status_df[status_df.time.dt.minute%5 ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b516026be82c82532174da0bd926264aabd576d4"
   },
   "outputs": [],
   "source": [
    "stations_df.rename(columns={\"id\":\"station_id\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "caf50ae0504ca9ce91becb492855cd10a5abb426"
   },
   "outputs": [],
   "source": [
    "stations_df.installation_date = pd.to_datetime(stations_df.installation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now merge with the stations dataframe on station_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d7ddd7ade0c3ec329911d035dd2fd35948d7b4f"
   },
   "outputs": [],
   "source": [
    "status_df = status_df.merge(stations_df,on=\"station_id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we the status dataframe with lat/lon and names from the station dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32e5c70ff38bf6fbad0f501b8885be08d5a85fbc"
   },
   "outputs": [],
   "source": [
    "status_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we drop the index colum in the status dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dcf33f8f85c1319a951c38a44ae3df18a8f582e7"
   },
   "outputs": [],
   "source": [
    "status_df.reset_index(inplace=True)\n",
    "status_df.drop(columns=[\"index\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1caff87b774fb84229a312ee975444313603ee8"
   },
   "outputs": [],
   "source": [
    "status_df[\"date\"] = status_df.time.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "965ce44b5909a691ccc0a26bb4bfbe1265e06eb0"
   },
   "outputs": [],
   "source": [
    "status_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load up the weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9fef5dbf680fc4f3d89a41b103cdbbcafc946f2"
   },
   "outputs": [],
   "source": [
    "weather_df = reduce_mem_usage(pd.read_csv('./weather.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ebce5bed8a66240197502b9848db787109515e35"
   },
   "outputs": [],
   "source": [
    "weather_df.date = pd.to_datetime(weather_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5fb149a6891be07be1ef5df9e08e579a19ff217"
   },
   "outputs": [],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbec994e4daa181c00c298053564e74c6fbf17fc"
   },
   "source": [
    "## Mapping of Zip Codes ans City names\n",
    "### 95113 - San Jose\n",
    "### 94301 - Palo Alto\n",
    "### 94107 - San Francisco\n",
    "\n",
    "### 94063 - Redwood City\n",
    "### 94041 - Mountain View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69629cea557c4322ca1366243b0b64c8524be317"
   },
   "outputs": [],
   "source": [
    "zipcode_city_dict = dict()\n",
    "zipcode_city_dict[95113] = 'San Jose'\n",
    "zipcode_city_dict[94301] = 'Palo Alto'\n",
    "zipcode_city_dict[94107] = 'San Francisco'\n",
    "zipcode_city_dict[94063] = 'Redwood City'\n",
    "zipcode_city_dict[94041] = 'Mountain View'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now map the zipcode to city name and apply it to the weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a4690b2b73d61aa49bfe9a80e7dfb0bbcd30622"
   },
   "outputs": [],
   "source": [
    "weather_df[\"city\"] = weather_df.zip_code.apply(lambda x:zipcode_city_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2535c987904c0c4782d3a5ff14d7b9bbb947f9e"
   },
   "outputs": [],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db75c0f0035fe27fccacd84b3a73498fdbe72d88"
   },
   "outputs": [],
   "source": [
    "status_df.date = pd.to_datetime(status_df.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets join the weather and status dataframes on date and city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94b4341718edd136d1b6e691e48e3e4f6be69eda"
   },
   "outputs": [],
   "source": [
    "status_df = status_df.merge(weather_df,how=\"left\",on=[\"date\",\"city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce0989037f01bbcdbe83fbdf220523367ee5d4e0"
   },
   "outputs": [],
   "source": [
    "status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1b3b626d871ee48dd7c05f254f5b27226bd8281"
   },
   "outputs": [],
   "source": [
    "status_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use LabelEncoder on events, precipitation_inches and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d51c616694280e72d2a3456ff6d0559a4456497"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77897ca8f69909cfeb72922ac673682d74163228"
   },
   "outputs": [],
   "source": [
    "status_df[\"events\"] = le.fit_transform(status_df[\"events\"])\n",
    "status_df[\"precipitation_inches\"] = le.fit_transform(status_df[\"precipitation_inches\"])\n",
    "status_df[\"name\"] = le.fit_transform(status_df[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d308c4fd3dd7a3d86a7aa82055c91dfe811697d5"
   },
   "outputs": [],
   "source": [
    "status_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create our train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecc16b70203e3ea36bebcf20eb23c8cfb94be298"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(len(status_df), 1))\n",
    "msk = np.random.rand(len(df)) < 0.6666\n",
    "status_df_train = status_df[msk]\n",
    "status_df_test = status_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e71bd29f175352bf4a2571c57000d91e495ec33"
   },
   "outputs": [],
   "source": [
    "train_cols = [c for c in status_df_train.columns if c not in ['time','installation_date','date','city','lat','long','name','bikes_available']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols.insert(0,'bikes_available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e9857783a3cc2b3b089162b7891a81a6522f21b"
   },
   "outputs": [],
   "source": [
    "train_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols = train_cols.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols.remove('bikes_available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-xgboost-churn\"\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "csv_buffer = StringIO()\n",
    "status_df_train[train_cols].to_csv(csv_buffer, header=False, index = False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'geo/train/train.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "status_df_test[train_cols].to_csv(csv_buffer, header=False, index = False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'geo/test/test.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets generate the files for AutoPilot in the next lab\n",
    "csv_buffer = StringIO()\n",
    "status_df_train[train_cols].to_csv(csv_buffer, index = False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'autopilot/train/train.csv').put(Body=csv_buffer.getvalue())\n",
    "csv_buffer = StringIO()\n",
    "status_df_test[test_cols].to_csv(csv_buffer, index = False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'autopilot/test/test.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'geo'\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/train\".format(bucket, prefix), content_type=\"csv\"\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/test/\".format(bucket, prefix), content_type=\"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=6,\n",
    "    eval_metric=\"rmse\",\n",
    "    silent=0,\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=100,\n",
    ")\n",
    "\n",
    "xgb.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for x in range(6):\n",
    "    random_row=status_df_test[train_cols].sample()\n",
    "    csv_input = random_row[test_cols].to_csv(header=False,index=False)\n",
    "    test_bikes_avail = random_row['bikes_available'].to_csv(header=False,index=False)\n",
    "    predicted_bikes_avail = xgb_predictor.predict(csv_input).decode(\"utf-8\")\n",
    "    print(f'Test {x} - \\n\\tPredicted Bike Available \\t{predicted_bikes_avail} \\n\\tActual Bike Available \\t\\t{test_bikes_avail}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6257827cb9d5f8a75cf0fee0af36ae72a7ff0c4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
